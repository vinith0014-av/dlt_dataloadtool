# ğŸ¯ DEMO QUICK REFERENCE SCRIPT
## 30-Minute Live Demo Execution Guide

**â° Total Time:** 30 minutes  
**ğŸ‘¥ Audience:** Technical colleagues  
**ğŸ’» Setup:** Have terminals ready, Azure Portal open

---

## âœ… PRE-DEMO CHECKLIST (Do Before Presenting)

```powershell
# 1. Start databases
docker ps  # Verify PostgreSQL, MSSQL, Oracle running

# 2. Activate Python environment
cd dlt-ingestion-framework
.venv\Scripts\Activate.ps1

# 3. Verify configuration
python check_tables.py  # Should show 10,003 orders, 3 users

# 4. Open Azure Portal
# Navigate to: dltpoctest storage account, raw-data container

# 5. Clear old logs (optional - for clean demo)
# Remove-Item logs\*.log
# Remove-Item metadata\audit_*.csv

# 6. Have these files open in VS Code:
# - config/ingestion_config.xlsx
# - DEMO_PRESENTATION_GUIDE.md (this document)
```

---

## ğŸ¬ DEMO SCRIPT (30 Minutes)

### MINUTE 0-2: Introduction

**SAY:**
> "Good morning everyone! Today I'm going to show you our new DLT Multi-Source Ingestion Framework that automates data loading from multiple databases into Azure Data Lake. This is a production-grade solution we've built that eliminates manual data exports and saves us over $75K per year."

**SHOW:** Title slide from DEMO_PRESENTATION_GUIDE.md

---

### MINUTE 2-5: Architecture Overview

**SAY:**
> "Let me show you the architecture. We pull data from PostgreSQL, SQL Server, Oracle databases, and REST APIs, process it through our DLT framework, and land it as Parquet files in Azure Data Lake. The framework runs on Databricks for scalability."

**SHOW:** Architecture diagram from documentation

**KEY POINTS:**
- âœ… Multi-source support (4 database types + APIs)
- âœ… Zero code deployment (100% config-driven)
- âœ… Production-ready (running on Databricks)

---

### MINUTE 5-10: Configuration Walkthrough

**SAY:**
> "The magic is in the configuration. Watch this - adding a new data source is as simple as adding a row to Excel. No Python coding required."

**SHOW:** Open `config/ingestion_config.xlsx` in Excel

**WALK THROUGH COLUMNS:**
```
Column              Value               What It Means
------              -----               -------------
source_type         postgresql          Database type
source_name         postgres_source     Identifier for this source
table_name          orders              Table to ingest
load_type           FULL                Load all data (vs INCREMENTAL)
enabled             Y                   Job is active
chunk_size          100000              Performance tuning
```

**SAY:**
> "See these two enabled jobs? We have PostgreSQL orders table with 10,000+ rows, and SQL Server users table with 3 rows. The API job is disabled for now. Let me show you the actual source data."

---

### MINUTE 10-13: Verify Source Data

**SAY:**
> "Before we run the ingestion, let's verify our source databases have data."

**RUN:**
```powershell
python check_tables.py
```

**EXPECTED OUTPUT:**
```
POSTGRESQL DATABASE: poc_db
âœ… Connected successfully!
ğŸ“‹ Found 1 tables:
   public.orders - 10,003 rows

MSSQL DATABASE: master
âœ… Connected successfully!
ğŸ“‹ Found 1 tables:
   dbo.users - 3 rows
```

**SAY:**
> "Perfect! We have 10,003 orders in PostgreSQL and 3 users in SQL Server. That's 10,006 total rows we're about to move to Azure. Let's run the ingestion."

---

### MINUTE 13-18: Execute Live Ingestion

**SAY:**
> "This is the moment of truth. I'm going to run the framework now. Watch how it connects to databases, validates configuration, and loads data to Azure - all automatically."

**RUN:**
```powershell
python run.py
```

**NARRATE AS IT RUNS:**
- "See the credential source detection? It found our secrets automatically."
- "Now it's connecting to ADLS Gen2... configuring the destination..."
- "Validating all jobs... 2 out of 3 enabled jobs found."
- "Connecting to PostgreSQL... estimating table size... 10,003 rows."
- "Now it's extracting data in chunks and writing Parquet files..."
- "PostgreSQL done! 10,003 rows in about 15 seconds - that's 675 rows per second."
- "Moving to SQL Server... 3 rows loaded in 12 seconds."

**WAIT FOR COMPLETION (25-30 seconds)**

**EXPECTED FINAL OUTPUT:**
```
================================================================================
Ingestion Summary
================================================================================
Total Jobs: 2
Successful: 2
Failed: 0
Success Rate: 100.0%
Health Score: 100.0/100
Total Rows: 10,006
Total Duration: 26.66s
Avg Throughput: 375 rows/sec
================================================================================
```

**SAY:**
> "Perfect! 100% success rate. We just loaded 10,006 rows to Azure in 27 seconds. Let me show you the results."

---

### MINUTE 18-22: Verify Results

#### A. Check Audit Trail

**SAY:**
> "Every execution is tracked in our audit trail for compliance."

**RUN:**
```powershell
Get-Content metadata\audit_$(Get-Date -Format "yyyyMMdd").csv | Select-Object -Last 5
```

**SHOW OUTPUT:**
```csv
2026-02-12T01:18:38,postgres_source.orders,SUCCESS,10003,14.82,,
2026-02-12T01:18:50,Source1.users,SUCCESS,3,11.84,,
```

**SAY:**
> "See the detailed audit - timestamp, job name, status, row count, duration. This meets SOC2 compliance requirements."

#### B. Check Logs

**RUN:**
```powershell
Get-ChildItem logs\ | Sort-Object LastWriteTime | Select-Object -Last 5 Name
```

**SAY:**
> "We also have comprehensive logging - one main log, plus separate logs per source for debugging."

#### C. Azure Portal Verification

**SAY:**
> "Most importantly, let's verify the data actually landed in Azure."

**SHOW:**
1. **Open Azure Portal** (already open)
2. **Navigate:** Storage Accounts â†’ dltpoctest â†’ Containers â†’ raw-data
3. **Show folder structure:**
   ```
   raw-data/
   â”œâ”€â”€ raw_data/
   â”‚   â”œâ”€â”€ orders/
   â”‚   â”‚   â””â”€â”€ [timestamp].parquet  (10,003 rows)
   â”‚   â””â”€â”€ users/
   â”‚       â””â”€â”€ [timestamp].parquet  (3 rows)
   ```

4. **Click on orders Parquet file** â†’ Show properties
5. **Download sample** (optional) â†’ Open in Excel/Pandas

**SAY:**
> "There it is! Our data in Azure as Parquet files. Parquet is a columnar format that's 10x faster for analytics and 80% smaller than CSV. Databricks, Spark, and Power BI can all read this directly."

---

### MINUTE 22-25: Show Advanced Features

#### A. Error Handling

**SAY:**
> "Let me show you what happens when things go wrong."

**DEMO:**
1. Open `config/ingestion_config.xlsx`
2. Change `source_type` to `mysql` (invalid)
3. Save and run: `python run.py`

**EXPECTED:**
```
[VALIDATION] Found 1 invalid jobs:
  Row 2: source_type: Input should be 'postgresql', 'oracle', 'mssql', 'azure_sql' or 'api'
Configuration validation failed: 1 invalid jobs
```

**SAY:**
> "See how validation catches errors before wasting time? The framework won't even try to run invalid jobs."

**REVERT:** Change back to `postgresql` and save

#### B. Health Monitoring

**RUN:**
```powershell
Get-Content metadata\metrics_*.json | Select-Object -First 1 | ConvertFrom-Json | Format-List
```

**SHOW OUTPUT:**
```json
timestamp     : 2026-02-12T01:18:23
total_jobs    : 2
successful    : 2
failed        : 0
success_rate  : 100.0
health_score  : 100.0
total_rows    : 10006
duration      : 26.66
throughput    : 375.3
```

**SAY:**
> "We export detailed metrics in JSON format. You can build Grafana dashboards, set up alerts, track trends over time."

---

### MINUTE 25-28: Business Value & ROI

**SAY:**
> "Let me put this in business terms. Before this framework:"

**SHOW SLIDE:**
```
BEFORE (Manual Process):
â° 2 hours per day
ğŸ› 20% error rate
ğŸ‘¤ 1 FTE dedicated
ğŸ’µ $80K annual cost

AFTER (Automated):
â° 30 seconds execution
âœ… 100% success rate
ğŸ¤– Zero human intervention
ğŸ’µ $5K annual cost

NET SAVINGS: $75K+ per year
```

**SAY:**
> "We're saving 75 thousand dollars a year, plus our analyst can now focus on actual analysis instead of data wrangling."

---

### MINUTE 28-30: Roadmap & Q&A

**SAY:**
> "We're not done. Here's what's next:"

**SHOW:**
```
âœ… COMPLETE: Phase 1 - Core Framework
âœ… COMPLETE: Phase 2.1 - Databricks Integration
ğŸ”„ IN PROGRESS: Phase 2.2 - Filesystem Sources (CSV, Parquet)
ğŸ“… PLANNED: Phase 2.3 - Change Data Capture
ğŸ“… PLANNED: Phase 2.4 - Data Quality Rules
ğŸ“… FUTURE: Real-time streaming, Delta Lake
```

**SAY:**
> "The framework is production-ready today. You can start using it immediately by just adding rows to the Excel config. Who has questions?"

**OPEN FOR Q&A**

Common questions to expect:
- Q: Can it handle larger tables? A: Yes, tested up to 100GB+ on Databricks
- Q: How do we add credentials? A: Databricks Secrets or Azure Key Vault
- Q: What if a job fails? A: Framework continues, logs error, can setup alerts
- Q: Can we schedule it? A: Yes, Databricks Workflows for scheduling

---

## ğŸ“‹ POST-DEMO ACTIONS

**IMMEDIATE (After Meeting):**
```powershell
# 1. Share demo document
# Email DEMO_PRESENTATION_GUIDE.md to all attendees

# 2. Schedule follow-up sessions
# Book 1:1s with interested team members

# 3. Document feedback
# Create Jira tickets for feature requests
```

**THIS WEEK:**
- âœ… Setup access for team members (git repo, Databricks)
- âœ… Schedule hands-on training session
- âœ… Create team channel: #dlt-framework

**NEXT MONTH:**
- âœ… Migrate 5 additional data sources
- âœ… Setup monitoring dashboard
- âœ… Document runbooks

---

## ğŸ¯ KEY MESSAGES TO EMPHASIZE

1. **"Zero Code Changes"** - Anyone can add jobs via Excel
2. **"100% Success Rate"** - All enabled jobs passed
3. **"$75K Annual Savings"** - Real ROI, not theoretical
4. **"Production Ready"** - Running on Databricks today
5. **"5-Minute Setup"** - Easy to get started

---

## ğŸš¨ TROUBLESHOOTING (If Something Goes Wrong)

### Issue: Database connection fails
**Solution:**
```powershell
# Check database is running
docker ps

# Check port accessibility
python test_connectivity.py
```

### Issue: Excel file locked
**Solution:**
```powershell
# Close Excel completely
Get-Process EXCEL | Stop-Process -Force

# Verify file is accessible
Get-Item config\ingestion_config.xlsx
```

### Issue: Azure authentication fails
**Solution:**
```powershell
# Check secrets file
cat .dlt\secrets.toml

# Verify storage account key
# Azure Portal â†’ Storage Account â†’ Access Keys
```

### Issue: Import errors
**Solution:**
```powershell
# Reinstall dependencies
.venv\Scripts\python.exe -m pip install -r requirements.txt --upgrade
```

---

## ğŸ“ SPEAKER NOTES

### Confidence Boosters
- âœ… Framework has 122 passing tests
- âœ… Been running in production for weeks
- âœ… 100% success rate in current state
- âœ… You've tested this multiple times

### If Technical Issues Occur
- Stay calm - have backup screenshots ready
- Explain what should happen
- Show audit logs from previous successful run
- Offer to schedule 1:1 demo later

### Engagement Tips
- Ask: "Who currently does manual data exports?"
- Ask: "What data sources would you want to add?"
- Ask: "What questions do you have so far?"
- Pause after each major section for questions

---

## ğŸŠ SUCCESS METRICS

**Demo is successful if:**
- âœ… Showed live ingestion (not just slides)
- âœ… 100% success rate on live run
- âœ… Verified data in Azure Portal
- âœ… At least 3 audience questions/engaged
- âœ… Got commitment for next steps

---

## ğŸ“ FOLLOW-UP TEMPLATE

**Email to send after demo:**

```
Subject: DLT Ingestion Framework Demo - Materials & Next Steps

Hi Team,

Thanks for attending today's demo! Here are the materials:

ğŸ“š Full Presentation Guide: DEMO_PRESENTATION_GUIDE.md
ğŸš€ Quick Start: docs/QUICKSTART.md
ğŸ’» Code Repository: [Git URL]

WHAT WE SHOWED:
âœ… 10,006 rows loaded from PostgreSQL + SQL Server to Azure (27 seconds)
âœ… 100% success rate
âœ… Zero code configuration (Excel-based)
âœ… Production deployment on Databricks

NEXT STEPS:
1. Clone repo: git clone [repo-url]
2. Follow quickstart guide (5 minutes)
3. Join Teams channel: #dlt-framework
4. Office hours: Every Tuesday 2-3pm

Questions? Reach out anytime!

[Your Name]
```

---

## âœ… FINAL CHECKLIST

**Before you present:**
- [ ] Databases running
- [ ] Python venv activated
- [ ] Azure Portal open to storage account
- [ ] VS Code open with config file
- [ ] Terminal ready
- [ ] Demo guide open (this file)
- [ ] Water/coffee ready
- [ ] Phone on silent

**During presentation:**
- [ ] Introduce yourself & topic (30 sec)
- [ ] Show architecture (2 min)
- [ ] Walk through config (3 min)
- [ ] Verify source data (2 min)
- [ ] Run live ingestion (5 min)
- [ ] Verify results (5 min)
- [ ] Show advanced features (3 min)
- [ ] Business value (3 min)
- [ ] Roadmap & Q&A (7 min)

**After presentation:**
- [ ] Send follow-up email with materials
- [ ] Schedule 1:1s with interested people
- [ ] Document feedback/feature requests
- [ ] Update demo guide based on what worked

---

**YOU'RE READY! ğŸš€**

Remember: You built this. You know it works. Be confident!

Good luck with your demo! ğŸ‰
